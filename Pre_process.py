import cv2
import os
import numpy as np
import mediapipe as mp

# Kh·ªüi t·∫°o Mediapipe Hands
mp_hands = mp.solutions.hands
mp_draw = mp.solutions.drawing_utils

# Kh·ªüi t·∫°o model ph√°t hi·ªán b√†n tay
hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5)

# ƒê·ªãnh nghƒ©a th∆∞ m·ª•c ƒë·∫ßu v√†o v√† ƒë·∫ßu ra
data_dir = r"D:\gesture_recognition\collected\dataNhi"
output_dir = "dataset_split/processed_dataNhi"
os.makedirs(output_dir, exist_ok=True)

# Th√¥ng s·ªë ti·ªÅn x·ª≠ l√Ω
offset = 20
crop_size = (224, 224)


# H√†m c√¢n b·∫±ng s√°ng/t∆∞∆°ng ph·∫£n
def enhance_contrast(img):
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    l = clahe.apply(l)
    img = cv2.merge([l, a, b])
    return cv2.cvtColor(img, cv2.COLOR_LAB2BGR)


# Duy·ªát qua t·ª´ng th∆∞ m·ª•c con trong data_dir
for category in os.listdir(data_dir):
    category_path = os.path.join(data_dir, category)

    if os.path.isdir(category_path):
        print(f"üîπ ƒêang x·ª≠ l√Ω th∆∞ m·ª•c: {category}")

        # T·∫°o th∆∞ m·ª•c ƒë·∫ßu ra t∆∞∆°ng ·ª©ng
        category_output_path = os.path.join(output_dir, category)
        os.makedirs(category_output_path, exist_ok=True)

        # Duy·ªát qua t·ª´ng file ·∫£nh trong th∆∞ m·ª•c
        for img_name in os.listdir(category_path):
            img_path = os.path.join(category_path, img_name)

            # ƒê·ªçc ·∫£nh
            img = cv2.imread(img_path)
            if img is None:
                print(f"‚ùå Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {img_path}")
                continue

            img = enhance_contrast(img)  # C√¢n b·∫±ng ƒë·ªô t∆∞∆°ng ph·∫£n

            # Chuy·ªÉn ·∫£nh sang kh√¥ng gian m√†u RGB
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

            # Ph√°t hi·ªán b√†n tay
            results = hands.process(img_rgb)

            # Ki·ªÉm tra n·∫øu ph√°t hi·ªán b√†n tay
            if not results.multi_hand_landmarks:
                print(f"‚ùå Kh√¥ng ph√°t hi·ªán b√†n tay: {img_name}")
                continue

            print(f"‚úÖ B√†n tay ƒë∆∞·ª£c ph√°t hi·ªán trong ·∫£nh: {img_name}")

            # V·∫Ω landmarks l√™n ·∫£nh
            for hand_landmarks in results.multi_hand_landmarks:
                mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)

            # L·∫•y t·ªça ƒë·ªô b√†n tay
            hand_landmarks = results.multi_hand_landmarks[0]
            h, w, _ = img.shape

            x_min = min([int(lm.x * w) for lm in hand_landmarks.landmark])
            y_min = min([int(lm.y * h) for lm in hand_landmarks.landmark])
            x_max = max([int(lm.x * w) for lm in hand_landmarks.landmark])
            y_max = max([int(lm.y * h) for lm in hand_landmarks.landmark])

            # M·ªü r·ªông v√πng c·∫Øt v·ªõi offset
            x1, y1 = max(0, x_min - offset), max(0, y_min - offset)
            x2, y2 = min(w, x_max + offset), min(h, y_max + offset)

            # Ki·ªÉm tra k√≠ch th∆∞·ªõc v√πng c·∫Øt
            if (x2 - x1) < 10 or (y2 - y1) < 10:
                print(f"‚ùå V√πng c·∫Øt qu√° nh·ªè: {img_name}")
                continue

            # C·∫Øt ·∫£nh b√†n tay
            imgCrop = img[y1:y2, x1:x2]

            # Resize v·ªÅ k√≠ch th∆∞·ªõc chu·∫©n (224x224)
            imgCrop = cv2.resize(imgCrop, crop_size, interpolation=cv2.INTER_AREA)

            # L∆∞u ·∫£nh ƒë√£ x·ª≠ l√Ω v√†o th∆∞ m·ª•c m·ªõi
            save_path = os.path.join(category_output_path, img_name)
            cv2.imwrite(save_path, imgCrop)
            print(f"üìÅ ·∫¢nh ƒë√£ l∆∞u: {save_path}")

print("üéØ Ho√†n t·∫•t! ·∫¢nh ƒë√£ x·ª≠ l√Ω ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c 'processed_dataMinh'.")
